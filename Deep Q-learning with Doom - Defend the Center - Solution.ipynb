{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-learning with Doom\n",
    "In this tutorial we will implement deep Q-learning to teach an agent to play Doom.\n",
    "\n",
    "We will use Keras for the deep learning part, and vizdoom to run doom in python.\n",
    "\n",
    "Here is a gif of the final result:\n",
    "\n",
    "<img src=\"figures/defend_the_center_dqn.gif\"/>\n",
    "\n",
    "## Prerequisites\n",
    "- [Python 3.6](https://www.python.org/downloads/release/python-367/)\n",
    "- pip install numpy gym tensorflow keras skimage\n",
    "- vizdoom:\n",
    "  - Download vizdoom from [here](https://github.com/mwydmuch/ViZDoom/releases/download/1.1.5pre/ViZDoom-1.1.5pre-Win-Python36-x86_64.zip) (for python 3.6)\n",
    "    - For other versions of python, see https://github.com/mwydmuch/ViZDoom/releases\n",
    "  - Extract and copy the `vizdoom` folder into site-packages:\n",
    "    - Python: python_root\\Lib\\site-packges\n",
    "    - Anaconda: anaconda_root\\lib\\pythonX.X\\site-packages\n",
    "    - If you don't know where your python installation is, run `where python` to see where it's installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import vizdoom\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Conv2D, Dense, Flatten, MaxPooling2D, Lambda\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import SGD\n",
    "from skimage import transform\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize DoomGame\n",
    "We will load the **defend_the_center** scenario.\n",
    "\n",
    "Hese is the summary of this scenario from https://github.com/mwydmuch/ViZDoom/tree/master/scenarios:\n",
    "\n",
    "\n",
    "> ## DEFEND THE CENTER\n",
    "> The purpose of this scenario is to teach the agent that killing the \n",
    "monsters is GOOD and when monsters kill you is BAD. In addition,\n",
    "wasting amunition is not very good either. Agent is rewarded only \n",
    "for killing monsters so he has to figure out the rest for himself.\n",
    "\n",
    "> Map is a large circle. Player is spawned in the exact center.\n",
    "5 melee-only, monsters are spawned along the wall. Monsters are \n",
    "killed after a single shot. After dying each monster is respawned \n",
    "after some time. Episode ends when the player dies (it's inevitable \n",
    "becuse of limitted ammo).\n",
    "\n",
    "> __REWARDS:__\n",
    "> +1 for killing a monster\n",
    "\n",
    "> Further configuration:\n",
    "* 3 available buttons: turn left, turn right, shoot (attack)\n",
    "* death penalty = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = vizdoom.DoomGame()\n",
    "game.load_config(\"scenarios/defend_the_center.cfg\")\n",
    "\n",
    "# Visualize the game (set to False to train faster)\n",
    "game.set_window_visible(True)\n",
    "\n",
    "# Set screen format to greyscale. This improves training time\n",
    "game.set_screen_format(vizdoom.ScreenFormat.GRAY8)\n",
    "\n",
    "# Make the game end after 2100 ticks (set to 0 to disable)\n",
    "game.set_episode_timeout(2100)\n",
    "\n",
    "# Init game\n",
    "game.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Keras Model\n",
    "## Let's Define some Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions: 3\n"
     ]
    }
   ],
   "source": [
    "num_episodes       = 500     # How many episodes to run\n",
    "num_actions        = game.get_available_buttons_size()\n",
    "replay_buffer_size = 10000   # How many experiences to store in our memory\n",
    "learning_rate      = 0.0002  # How \"fast\" should we update the network (alpha)\n",
    "discount_factor    = 0.95    # Future reward discount factor (gamma)\n",
    "batch_size         = 64      # How many replays should we use for training\n",
    "enable_training    = True    # Should we train the agent?\n",
    "\n",
    "print(\"Number of actions:\", num_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the network\n",
    "\n",
    "Here we use Keras to construct the following network:\n",
    "\n",
    "<img src=\"figures/Deep_Q_learning_model.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 84, 84, 4)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 82, 82, 8)    296         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 41, 41, 8)    0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 39, 39, 32)   2336        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 19, 19, 32)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 11552)        0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          5915136     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          65664       dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 3)            387         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1)            0           dense_3[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,983,819\n",
      "Trainable params: 5,983,819\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_images = Input(shape=(84, 84, 4))\n",
    "input_action = Input(shape=(num_actions,)) # one-hot vector\n",
    "x = Conv2D(8, (3, 3), activation='elu', padding=\"valid\", input_shape=(84, 84, 4))(input_images)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Conv2D(32, (3, 3), activation='elu', padding=\"valid\")(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(512, activation='elu')(x)\n",
    "x = Dense(128, activation='elu')(x)\n",
    "Q_actions = Dense(game.get_available_buttons_size(), activation=None)(x)\n",
    "Q_input_action = Lambda(lambda x: K.expand_dims(K.sum(x[0] * x[1], axis=1), axis=-1))([Q_actions, input_action]) # Get Q-predicted for input_action\n",
    "\n",
    "# Create one model for training and one for prediction\n",
    "train_model = Model(inputs=[input_images, input_action], outputs=[Q_input_action])\n",
    "train_model.compile(loss=\"mse\", optimizer=SGD(lr=learning_rate))\n",
    "train_model.summary()\n",
    "predict_model = Model(inputs=[input_images], outputs=[Q_actions])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "\n",
    "Remember that we want to use our model to estimate a Q-value for every action in a state, like this:\n",
    "\n",
    "<img src=\"figures/Deep_Q_figure_1.png\" style=\"width: 700px;\"/>\n",
    "\n",
    "Where $Q(s_t, a_0)$ represents how good it is to take action $a_0$ in state $s_t$ according to our network.\n",
    "\n",
    "So the idea here is to train the network to make correct prediction of how good certain actions are, by optimizing the parameters in our network through exploration of the environment.\n",
    "\n",
    "This can be achieved with the Deep Q-learning algorithm:\n",
    "\n",
    "<img src=\"figures/Deep_Q_algorithm.png\" style=\"width: 700px;\"/>\n",
    "\n",
    "Or alternatively:\n",
    "\n",
    "```\n",
    "Initialize replay buffer D with size N\n",
    "Set the batch size B to some constant\n",
    "for every episode:\n",
    "    Reset environment\n",
    "    Get initial state s\n",
    "    for every step:\n",
    "        Predict Q_values for state s\n",
    "        Set a to the action with the highest Q_value\n",
    "        Perform action a get reward r\n",
    "        if game not is over:\n",
    "            Get new state s'\n",
    "        else:\n",
    "            Set s' to None\n",
    "        Store experience in D as a tuple <s, a, r, s'>\n",
    "        if N >= B:\n",
    "            Set V to equal random sample of B elements from D\n",
    "            Q_target = V.r + gamma * max_a' Q(V.s', a') if V.s' is not None else V.r\n",
    "            Train network with V.s as inputs and Q_target as target\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function preprocesses a frame from the game by:\n",
    "# - Remove the ceiling and floor pixels\n",
    "# - Normalize the pixel values to [0, 1] range\n",
    "# - Resize the image to 84x84 pixels\n",
    "def preprocess_frame(frame):\n",
    "    cropped_frame = frame[30:-10, 30:-30]                             # Crop the screen\n",
    "    normalized_frame = cropped_frame / 255.0                          # Normalize pixel values    \n",
    "    preprocessed_frame = transform.resize(normalized_frame, [84, 84]) # Resize\n",
    "    return preprocessed_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some useful functions:\n",
    "\n",
    "* `np.argmax(v)`\n",
    "  * Return the index of the largest value in v\n",
    "* `np.stack(list)`\n",
    "  * Stack every element in list and convert the stacked list into a matrix\n",
    "* `np.expand_dims(m, axis=n)`\n",
    "  * Expand matrix m with an additional dimension along axis n\n",
    "* `game.new_episode()`\n",
    "  * Start a new episode\n",
    "* `game.get_state().screen_buffer`\n",
    "  * Screen buffer (image) of the current state of the game\n",
    "* `game.make_action(actions)`\n",
    "  * Take the actions denoted by True in the actions vector (e.g. [True, False, False] to perform action 0)\n",
    "* `game.is_episode_finished()`\n",
    "  * Returns True when the episode is done (terminal state or episode timeout)\n",
    "* `random.sample(list, n)`\n",
    "  * Sample n elements from list\n",
    "* `predict_model.predict_on_batch(batch)`\n",
    "  * Feed batch through the network and return the output values (Q-values for the state)\n",
    "* `train_model.train_on_batch(batch, target)`\n",
    "  * Feed batch through the network and optimize network to predict \"target\" next time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Episode 64/500 --\n",
      "Episode loss: 1.7479450501414249\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e31e181c254d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mreplay_next_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                         \u001b[0mQ_next_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_next_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m                         \u001b[0mQ_next_max\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_next_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                         \u001b[0mQ_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_reward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdiscount_factor\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mQ_next_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict_on_batch\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1272\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_predict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1274\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1275\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if enable_training:\n",
    "    # Initialize replay buffer\n",
    "    replay_buffer = deque(maxlen=replay_buffer_size)\n",
    "\n",
    "    # Initialize frame stack\n",
    "    frame_stack = deque(maxlen=4)\n",
    "\n",
    "    # For every episode\n",
    "    episode_loss = float(\"nan\")\n",
    "    for episode in range(num_episodes):\n",
    "        clear_output(wait=True)\n",
    "        print(\"-- Episode {}/{} --\".format(episode+1, num_episodes))\n",
    "        print(\"Episode loss:\", episode_loss)\n",
    "\n",
    "        # Start new episode\n",
    "        game.new_episode()\n",
    "\n",
    "        # Initialize frame stack with the first frame of the game\n",
    "        initial_frame = preprocess_frame(game.get_state().screen_buffer)\n",
    "        for _ in range(4):\n",
    "            frame_stack.append(initial_frame)\n",
    "        state = np.stack(frame_stack, axis=2) # Stack the frames to setup the inital state\n",
    "\n",
    "        # Run the episode\n",
    "        episode_loss = 0\n",
    "        while not game.is_episode_finished():\n",
    "            # Get action with highest Q-value for current state\n",
    "            action = np.argmax(predict_model.predict_on_batch(np.expand_dims(state, axis=0)))\n",
    "            action_one_hot = [False] * num_actions\n",
    "            action_one_hot[action] = True\n",
    "\n",
    "            # Take action and get a reward\n",
    "            reward = game.make_action(action_one_hot)\n",
    "            \n",
    "            # If game is not finished\n",
    "            if not game.is_episode_finished():\n",
    "                # Get new state\n",
    "                frame_stack.append(preprocess_frame(game.get_state().screen_buffer))\n",
    "                new_state = np.stack(frame_stack, axis=2)\n",
    "            else:\n",
    "                new_state = None\n",
    "\n",
    "            # Store the replay\n",
    "            replay_buffer.append((state, action_one_hot, reward, new_state))\n",
    "            state = new_state\n",
    "\n",
    "            # Train network on a random sample of previous expreiences\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                # Get replay batch\n",
    "                replay_batch      = random.sample(replay_buffer, batch_size)\n",
    "                replay_state      = [r[0] for r in replay_batch]\n",
    "                replay_action     = [r[1] for r in replay_batch]\n",
    "                replay_reward     = [r[2] for r in replay_batch]\n",
    "                replay_next_state = [r[3] for r in replay_batch]\n",
    "\n",
    "                # Q_target = reward + gamma * max_a' Q(s')\n",
    "                Q_target = []\n",
    "                for i in range(batch_size):\n",
    "                    if replay_next_state[i] is not None:\n",
    "                        Q_next_state = predict_model.predict_on_batch(np.expand_dims(replay_next_state[i], axis=0))[0]\n",
    "                        Q_next_max   = np.max(Q_next_state)\n",
    "                        Q_target.append(replay_reward[i] + discount_factor * Q_next_max)\n",
    "                    else:\n",
    "                        Q_target.append(replay_reward[i])\n",
    "                \n",
    "                # Run training pass\n",
    "                episode_loss += train_model.train_on_batch([replay_state, replay_action], Q_target)\n",
    "    predict_model.save(\"defend_the_center_dqn.h5\")\n",
    "else:\n",
    "    predict_model = load_model(\"defend_the_center_dqn.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialize game with set_window_visible = True\n",
    "game = vizdoom.DoomGame()\n",
    "game.load_config(\"scenarios/defend_the_center.cfg\")\n",
    "\n",
    "# Visualize the game\n",
    "game.set_window_visible(True)\n",
    "\n",
    "# Set screen format to greyscale. This improves training time\n",
    "game.set_screen_format(vizdoom.ScreenFormat.GRAY8)\n",
    "\n",
    "# Make the game end after 2100 ticks (set to 0 to disable)\n",
    "game.set_episode_timeout(2100)\n",
    "\n",
    "# Init game\n",
    "game.init()\n",
    "\n",
    "# Saves a GIF when true\n",
    "record = False\n",
    "if record:\n",
    "    import imageio\n",
    "    recording = []\n",
    "    \n",
    "# Initialize frame stack\n",
    "frame_stack = deque(maxlen=4)\n",
    "\n",
    "# For every episode\n",
    "for episode in range(1):\n",
    "    # Start new episode\n",
    "    game.new_episode()\n",
    "    \n",
    "    # Initialize frame stack with the first frame of the game\n",
    "    initial_frame = preprocess_frame(game.get_state().screen_buffer)\n",
    "    for _ in range(4):\n",
    "        frame_stack.append(initial_frame)\n",
    "    state = np.stack(frame_stack, axis=2) # Stack the frames to setup the inital state\n",
    "    \n",
    "    # Run the episode\n",
    "    while not game.is_episode_finished():\n",
    "        if record:\n",
    "            recording.append((transform.resize(game.get_state().screen_buffer, [240, 320]) * 255.5).astype(np.uint8))\n",
    "        \n",
    "        # Get action with highest Q-value for current state\n",
    "        action = np.argmax(predict_model.predict_on_batch(np.expand_dims(state, axis=0)))\n",
    "        action_one_hot = [False] * num_actions\n",
    "        action_one_hot[action] = True\n",
    "        \n",
    "        # Take action and get a reward\n",
    "        reward = game.make_action(action_one_hot)\n",
    "        \n",
    "        # Break if the episode is finished\n",
    "        if game.is_episode_finished():\n",
    "            break\n",
    "        \n",
    "        # If not, get the new state\n",
    "        frame_stack.append(preprocess_frame(game.get_state().screen_buffer))\n",
    "        state = np.stack(frame_stack, axis=2)\n",
    "        \n",
    "if record:\n",
    "    imageio.mimwrite(\"defend_the_center_dqn.gif\", recording, subrectangles=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
