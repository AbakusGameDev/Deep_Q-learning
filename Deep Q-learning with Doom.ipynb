{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-learning with Doom\n",
    "In this tutorial we will implement deep Q-learning to teach an agent to play Doom.\n",
    "\n",
    "We will use Keras for the deep learning part, and vizdoom to run doom in python.\n",
    "\n",
    "TODO: Insert GIF of final result\n",
    "\n",
    "## Prerequisites\n",
    "- python3.7\n",
    "- pip install numpy pyplot gym tensorflow keras skimage\n",
    "- vizdoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import vizdoom\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.layers import Conv2D, Dense, Flatten, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from skimage import transform\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize DoomGame\n",
    "We will load the **defend_the_center** scenario.\n",
    "\n",
    "Hese is the summary of this scenario from https://github.com/mwydmuch/ViZDoom/tree/master/scenarios:\n",
    "\n",
    "> The purpose of this scenario is to teach the agent that killing the monsters is GOOD and when monsters kill you is BAD. In addition, wasting amunition is not very good either. Agent is rewarded only for killing monsters so he has to figure out the rest for himself.\n",
    "\n",
    "> Map is a large circle. Player is spawned in the exact center. 5 melee-only, monsters are spawned along the wall. Monsters are killed after a single shot. After dying each monster is respawned after some time. Episode ends when the player dies (it's inevitable becuse of limitted ammo).\n",
    "\n",
    "> REWARDS: +1 for killing a monster\n",
    "\n",
    "> Further configuration:\n",
    "\n",
    "> 3 available buttons: turn left, turn right, shoot (attack)\n",
    "\n",
    "> death penalty = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game = vizdoom.DoomGame()\n",
    "game.load_config(\"scenarios/defend_the_center.cfg\")\n",
    "\n",
    "# Visualize the game (set to False to train faster)\n",
    "game.set_window_visible(False)\n",
    "\n",
    "# Set screen format to greyscale. This improves training time\n",
    "game.set_screen_format(vizdoom.ScreenFormat.GRAY8)\n",
    "\n",
    "# Make the game end after 2100 ticks (set to 0 to disable)\n",
    "game.set_episode_timeout(2100)\n",
    "\n",
    "# Init game\n",
    "game.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Keras Model\n",
    "## Let's Define some Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions: 3\n"
     ]
    }
   ],
   "source": [
    "num_episodes       = 500     # How many episodes to run\n",
    "num_actions        = game.get_available_buttons_size()\n",
    "replay_buffer_size = 4000    # How many experiences to store in our memory\n",
    "learning_rate      = 0.01    # How \"fast\" should we update the network (alpha)\n",
    "discount_factor    = 0.95    # Future reward discount factor (gamma)\n",
    "batch_size         = 64      # How many replays should we use for training\n",
    "\n",
    "print(\"Number of actions:\", num_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the network\n",
    "\n",
    "Here we use Keras to construct the following network:\n",
    "\n",
    "<img src=\"figures/Deep_Q_learning_model.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 82, 82, 8)         296       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 41, 41, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 39, 39, 32)        2336      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 48672)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               6230144   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 6,241,227\n",
      "Trainable params: 6,241,227\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(8, (3, 3), activation='elu', padding=\"valid\", input_shape=(84, 84, 4)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), activation='elu', padding=\"valid\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='elu'))\n",
    "model.add(Dense(64, activation='elu'))\n",
    "model.add(Dense(game.get_available_buttons_size(), activation=None))\n",
    "model.summary()\n",
    "model.compile(loss=\"mse\", optimizer=SGD(lr=learning_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "\n",
    "Remember that we want to use our model to estimate a Q-value for every action in a state, like this:\n",
    "\n",
    "<img src=\"figures/Deep_Q_figure_1.png\" style=\"width: 700px;\"/>\n",
    "\n",
    "Where $Q(s_t, a_0)$ represents how good it is to take action $a_0$ in state $s_t$ according to our network.\n",
    "\n",
    "So the idea here is to train the network to make correct prediction of how good certain actions are, by optimizing the parameters in our network through exploration of the environment.\n",
    "\n",
    "This can be achieved with the Deep Q-learning algorithm:\n",
    "\n",
    "<img src=\"figures/Deep_Q_algorithm.png\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function preprocesses a frame from the game by:\n",
    "# - Remove the ceiling and floor pixels\n",
    "# - Normalize the pixel values to [0, 1] range\n",
    "# - Resize the image to 84x84 pixels\n",
    "def preprocess_frame(frame):\n",
    "    cropped_frame = frame[30:-10, 30:-30]                             # Crop the screen\n",
    "    normalized_frame = cropped_frame / 255.0                          # Normalize pixel values    \n",
    "    preprocessed_frame = transform.resize(normalized_frame, [84, 84]) # Resize\n",
    "    return preprocessed_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Episode 7/500 --\n",
      "Episode loss: 0.8093927421723492\n"
     ]
    }
   ],
   "source": [
    "# Initialize replay buffer\n",
    "replay_buffer = deque(maxlen=replay_buffer_size)\n",
    "\n",
    "# Initialize frame stack\n",
    "frame_stack = deque(maxlen=4)\n",
    "\n",
    "# For every episode\n",
    "episode_loss = float(\"nan\")\n",
    "for episode in range(num_episodes):\n",
    "    clear_output(wait=True)\n",
    "    print(\"-- Episode {}/{} --\".format(episode+1, num_episodes))\n",
    "    print(\"Episode loss:\", episode_loss)\n",
    "    \n",
    "    # Start new episode\n",
    "    game.new_episode()\n",
    "    \n",
    "    # Initialize frame stack with the first frame of the game\n",
    "    initial_frame = preprocess_frame(game.get_state().screen_buffer)\n",
    "    for _ in range(4):\n",
    "        frame_stack.append(initial_frame)\n",
    "    state = np.stack(frame_stack, axis=2) # Stack the frames to setup the inital state\n",
    "    \n",
    "    # Run the episode\n",
    "    episode_loss = 0\n",
    "    while not game.is_episode_finished():    \n",
    "        # Get action with highest Q-value for current state\n",
    "        action = np.argmax(model.predict_on_batch(np.expand_dims(state, axis=0)))\n",
    "        action_one_hot = [False] * num_actions\n",
    "        action_one_hot[action] = True\n",
    "        \n",
    "        # Take action and get a reward\n",
    "        reward = game.make_action(action_one_hot)\n",
    "        \n",
    "        # Break if the episode is finished\n",
    "        if game.is_episode_finished():\n",
    "            break\n",
    "        \n",
    "        # If not, get the new state\n",
    "        frame_stack.append(preprocess_frame(game.get_state().screen_buffer))\n",
    "        new_state = np.stack(frame_stack, axis=2)\n",
    "\n",
    "        # Store the replay\n",
    "        replay_buffer.append((state, action, reward, new_state))\n",
    "        state = new_state\n",
    "\n",
    "        # Train network on a random sample of previous expreiences\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            # Get replay batch\n",
    "            replay_batch      = random.sample(replay_buffer, batch_size)\n",
    "            replay_state      = np.array([r[0] for r in replay_batch])\n",
    "            replay_reward     = np.array([r[2] for r in replay_batch])\n",
    "            replay_next_state = np.array([r[3] for r in replay_batch])\n",
    "\n",
    "            # Q_target = reward + gamma * max_a' Q(s', a')\n",
    "            Q_target = np.expand_dims(replay_reward, axis=1) + discount_factor * model.predict_on_batch(replay_next_state)\n",
    "\n",
    "            # Run training pass\n",
    "            episode_loss += model.train_on_batch(replay_state, Q_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"defend_the_center_dqn.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reinitialize game with set_window_visible = True\n",
    "game = vizdoom.DoomGame()\n",
    "game.load_config(\"scenarios/defend_the_center.cfg\")\n",
    "\n",
    "# Visualize the game\n",
    "game.set_window_visible(True)\n",
    "\n",
    "# Set screen format to greyscale. This improves training time\n",
    "game.set_screen_format(vizdoom.ScreenFormat.GRAY8)\n",
    "\n",
    "# Make the game end after 2100 ticks (set to 0 to disable)\n",
    "game.set_episode_timeout(2100)\n",
    "\n",
    "# Init game\n",
    "game.init()\n",
    "\n",
    "# For every episode\n",
    "for episode in range(10):\n",
    "    # Start new episode\n",
    "    game.new_episode()\n",
    "    \n",
    "    # Initialize frame stack with the first frame of the game\n",
    "    initial_frame = preprocess_frame(game.get_state().screen_buffer)\n",
    "    for _ in range(4):\n",
    "        frame_stack.append(initial_frame)\n",
    "    state = np.stack(frame_stack, axis=2) # Stack the frames to setup the inital state\n",
    "    \n",
    "    # Run the episode\n",
    "    while not game.is_episode_finished():    \n",
    "        # Get action with highest Q-value for current state\n",
    "        action = np.argmax(model.predict_on_batch(np.expand_dims(state, axis=0)))\n",
    "        action_one_hot = [False] * num_actions\n",
    "        action_one_hot[action] = True\n",
    "        \n",
    "        # Take action and get a reward\n",
    "        reward = game.make_action(action_one_hot)\n",
    "        \n",
    "        # Break if the episode is finished\n",
    "        if game.is_episode_finished():\n",
    "            break\n",
    "        \n",
    "        # If not, get the new state\n",
    "        frame_stack.append(preprocess_frame(game.get_state().screen_buffer))\n",
    "        state = np.stack(frame_stack, axis=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
