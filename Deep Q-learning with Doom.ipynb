{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-learning with Doom\n",
    "In this tutorial we will implement deep Q-learning to teach an agent to play Doom.\n",
    "\n",
    "We will use Keras for the deep learning part, and vizdoom to run doom in python.\n",
    "\n",
    "## Prerequisites\n",
    "- python3.7\n",
    "- pip install numpy pyplot gym tensorflow keras skimage\n",
    "- vizdoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import vizdoom\n",
    "import os\n",
    "import time\n",
    "import keras\n",
    "import random\n",
    "from keras.layers import Conv2D, Dense, Flatten, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "from skimage import transform\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize DoomGame\n",
    "We will load the **defend_the_center** scenario.\n",
    "\n",
    "Hese is the summary of this scenario from https://github.com/mwydmuch/ViZDoom/tree/master/scenarios:\n",
    "\n",
    "> The purpose of this scenario is to teach the agent that killing the monsters is GOOD and when monsters kill you is BAD. In addition, wasting amunition is not very good either. Agent is rewarded only for killing monsters so he has to figure out the rest for himself.\n",
    "\n",
    "> Map is a large circle. Player is spawned in the exact center. 5 melee-only, monsters are spawned along the wall. Monsters are killed after a single shot. After dying each monster is respawned after some time. Episode ends when the player dies (it's inevitable becuse of limitted ammo).\n",
    "\n",
    "> REWARDS: +1 for killing a monster\n",
    "\n",
    "> Further configuration:\n",
    "\n",
    "> 3 available buttons: turn left, turn right, shoot (attack)\n",
    "\n",
    "> death penalty = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game = vizdoom.DoomGame()\n",
    "game.load_config(\"scenarios/defend_the_center.cfg\")\n",
    "\n",
    "# Visualize the game (set to False to train faster)\n",
    "game.set_window_visible(True)\n",
    "\n",
    "# Set screen format to greyscale\n",
    "# Improves training time\n",
    "game.set_screen_format(vizdoom.ScreenFormat.GRAY8)\n",
    "\n",
    "# Make the game end after 2100 ticks (set to 0 to disable)\n",
    "game.set_episode_timeout(2100)\n",
    "\n",
    "# Init game\n",
    "game.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Keras Model\n",
    "## Let's Define some Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_episodes       = 500\n",
    "num_steps          = 100\n",
    "num_actions        = game.get_available_buttons_size()\n",
    "replay_buffer_size = 1000000\n",
    "learning_rate      = 0.001\n",
    "discount_factor    = 0.7\n",
    "batch_size         = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 82, 82, 32)        1184      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 41, 41, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 39, 39, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 19, 19, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 17, 17, 128)       73856     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 36992)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               18940416  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 19,100,003\n",
      "Trainable params: 19,100,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='elu', padding=\"valid\", input_shape=(84, 84, 4)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='elu', padding=\"valid\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='elu', padding=\"valid\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='elu'))\n",
    "model.add(Dense(128, activation='elu'))\n",
    "model.add(Dense(game.get_available_buttons_size(), activation=None))\n",
    "model.summary()\n",
    "model.compile(loss=\"mse\", optimizer=SGD(lr=learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    cropped_frame = frame[30:-10, 30:-30]                             # Crop the screen\n",
    "    normalized_frame = cropped_frame / 255.0                          # Normalize pixel values    \n",
    "    preprocessed_frame = transform.resize(normalized_frame, [84, 84]) # Resize\n",
    "    return preprocessed_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Episode 499/500 --\n",
      "\n",
      "Episode loss: 0\n"
     ]
    }
   ],
   "source": [
    "# Initialize replay deque\n",
    "replay_buffer = deque(maxlen=replay_buffer_size)\n",
    "\n",
    "# Initialize frame stack\n",
    "frame_stack = deque(maxlen=4)\n",
    "\n",
    "# For every episode\n",
    "episode_loss = float(\"nan\")\n",
    "for episode in range(num_episodes):\n",
    "    clear_output(wait=True)\n",
    "    print(\"-- Episode {}/{} --\\n\".format(episode, num_episodes))\n",
    "    print(\"Episode loss:\", episode_loss)\n",
    "    \n",
    "    # Start new episode\n",
    "    game.new_episode()\n",
    "    \n",
    "    # Initialize frame stack with the first frame of the game\n",
    "    initial_frame = preprocess_frame(game.get_state().screen_buffer)\n",
    "    for _ in range(4):\n",
    "        frame_stack.append(initial_frame)\n",
    "    state = np.stack(frame_stack, axis=2) # Stack the frames to setup the inital state\n",
    "    \n",
    "    episode_loss = 0\n",
    "    while game.is_episode_finished():    \n",
    "        # Get action with highest Q-value for current state\n",
    "        action = np.argmax(model.predict_on_batch(np.expand_dims(state, axis=0)))\n",
    "        action_one_hot = [False] * num_actions\n",
    "        action_one_hot[action] = True\n",
    "        \n",
    "        # Take action and get reward\n",
    "        reward = game.make_action(action_one_hot)\n",
    "        \n",
    "        # Break if the episode is finished\n",
    "        if game.is_episode_finished():\n",
    "            break\n",
    "            \n",
    "        # Episode is not fished\n",
    "        # Get new state\n",
    "        frame_stack.append(preprocess_frame(game.get_state().screen_buffer))\n",
    "        new_state = np.stack(frame_stack, axis=2)\n",
    "\n",
    "        # Store the experience\n",
    "        replay_buffer.append((state, action, reward, new_state))\n",
    "        state = new_state\n",
    "\n",
    "        # Train network on a random sample of previous expreiences\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            # Get replay batch\n",
    "            replay_batch      = random.sample(replay_buffer, batch_size)\n",
    "            replay_state      = np.array([r[0] for r in replay_batch])\n",
    "            replay_reward     = np.array([r[2] for r in replay_batch])\n",
    "            replay_next_state = np.array([r[3] for r in replay_batch])\n",
    "\n",
    "            # Q_target = reward + gamma * max_a' Q(s')\n",
    "            Q_target = np.expand_dims(replay_reward, axis=1) + discount_factor * model.predict_on_batch(replay_next_state)\n",
    "\n",
    "            # Run training pass\n",
    "            loss += model.train_on_batch(replay_state, Q_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for episode in range(10):\n",
    "    game.new_episode()\n",
    "    while game.is_episode_finished():\n",
    "        action_one_hot = [False] * game.get_available_buttons_size()\n",
    "        #action_one_hot[action] = True\n",
    "        \n",
    "        # Take action and get reward\n",
    "        reward = game.make_action(action_one_hot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
