{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-learning with Doom\n",
    "In this tutorial we will implement deep Q-learning to teach an agent to play Doom.\n",
    "\n",
    "We will use Keras for the deep learning part, and vizdoom to run doom in python.\n",
    "\n",
    "Here is a gif of the final results:\n",
    "\n",
    "TODO:\n",
    "* DQN Psudeocode\n",
    "* Install instructions\n",
    "* Worksheet version of this notebook\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "- Python 3.7\n",
    "- pip install numpy pyplot gym tensorflow keras skimage\n",
    "- vizdoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import vizdoom\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.layers import Conv2D, Dense, Flatten, MaxPooling2D\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.optimizers import SGD\n",
    "from skimage import transform\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize DoomGame\n",
    "We will load the **basic** scenario.\n",
    "\n",
    "Hese is the summary of this scenario from https://github.com/mwydmuch/ViZDoom/tree/master/scenarios:\n",
    "\n",
    "\n",
    "> ## BASIC\n",
    "> The purpose of the scenario is just to check if using this\n",
    "framework to train some AI i 3D environment is feasible.\n",
    "\n",
    "> Map is a rectangle with gray walls, ceiling and floor.\n",
    "Player is spawned along the longer wall, in the center.\n",
    "A red, circular monster is spawned randomly somewhere along\n",
    "the opposite wall. Player can only (config) go left/right \n",
    "and shoot. 1 hit is enough to kill the monster. Episode \n",
    "finishes when monster is killed or on timeout.\n",
    "\n",
    "> __REWARDS:__\n",
    "\n",
    "> +101 for killing the monster\n",
    " -5 for missing\n",
    "Episode ends after killing the monster or on timeout.\n",
    "\n",
    "> Further configuration:\n",
    "* living reward = -1,\n",
    "* 3 available buttons: move left, move right, shoot (attack)\n",
    "* timeout = 300\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game = vizdoom.DoomGame()\n",
    "game.load_config(\"scenarios/basic.cfg\")\n",
    "\n",
    "# Visualize the game (set to False to train faster)\n",
    "game.set_window_visible(False)\n",
    "\n",
    "# Set screen format to greyscale. This improves training time\n",
    "game.set_screen_format(vizdoom.ScreenFormat.GRAY8)\n",
    "\n",
    "# Make the game end after 100 ticks (set to 0 to disable)\n",
    "game.set_episode_timeout(100)\n",
    "\n",
    "# Init game\n",
    "game.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Keras Model\n",
    "## Let's Define some Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions: 3\n"
     ]
    }
   ],
   "source": [
    "num_episodes       = 500     # How many episodes to run\n",
    "num_actions        = game.get_available_buttons_size()\n",
    "replay_buffer_size = 4000    # How many experiences to store in our memory\n",
    "learning_rate      = 0.0001  # How \"fast\" should we update the network (alpha)\n",
    "discount_factor    = 0.95    # Future reward discount factor (gamma)\n",
    "batch_size         = 64      # How many replays should we use for training\n",
    "enable_training    = True    # Should we train the agent?\n",
    "\n",
    "print(\"Number of actions:\", num_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the network\n",
    "\n",
    "Here we use Keras to construct the following network:\n",
    "\n",
    "<img src=\"figures/Deep_Q_learning_model.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 82, 82, 8)         296       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 41, 41, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 39, 39, 32)        2336      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 48672)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               6230144   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 6,241,227\n",
      "Trainable params: 6,241,227\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(8, (3, 3), activation='elu', padding=\"valid\", input_shape=(84, 84, 4)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), activation='elu', padding=\"valid\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='elu'))\n",
    "model.add(Dense(64, activation='elu'))\n",
    "model.add(Dense(game.get_available_buttons_size(), activation=None))\n",
    "model.summary()\n",
    "model.compile(loss=\"mse\", optimizer=SGD(lr=learning_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "\n",
    "Remember that we want to use our model to estimate a Q-value for every action in a state, like this:\n",
    "\n",
    "<img src=\"figures/Deep_Q_figure_1.png\" style=\"width: 700px;\"/>\n",
    "\n",
    "Where $Q(s_t, a_0)$ represents how good it is to take action $a_0$ in state $s_t$ according to our network.\n",
    "\n",
    "So the idea here is to train the network to make correct prediction of how good certain actions are, by optimizing the parameters in our network through exploration of the environment.\n",
    "\n",
    "This can be achieved with the Deep Q-learning algorithm:\n",
    "\n",
    "<img src=\"figures/Deep_Q_algorithm.png\" style=\"width: 700px;\"/>\n",
    "\n",
    "Or alternatively:\n",
    "\n",
    "```\n",
    "Initialize replay buffer D with size N\n",
    "Set the batch size B to some constant\n",
    "for every episode:\n",
    "    Reset environment\n",
    "    Get initial state s\n",
    "    for every step:\n",
    "        Predict Q_values for state s\n",
    "        Set a to the action with the highest Q_value\n",
    "        Perform action a get reward r\n",
    "        if game is over:\n",
    "            break\n",
    "        Get new state s'\n",
    "        Store experience in D as a tuple <s, a, r, s'>\n",
    "        if N >= B:\n",
    "            Set V to equal random sample of B elements from D\n",
    "            Calculate Q_target = V.r + gamma * max_a' Q(V.s', a')\n",
    "            Train network with V.s as inputs and Q_target as target\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function preprocesses a frame from the game by:\n",
    "# - Remove the ceiling and floor pixels\n",
    "# - Normalize the pixel values to [0, 1] range\n",
    "# - Resize the image to 84x84 pixels\n",
    "def preprocess_frame(frame):\n",
    "    cropped_frame = frame[30:-10, 30:-30]                             # Crop the screen\n",
    "    normalized_frame = cropped_frame / 255.0                          # Normalize pixel values    \n",
    "    preprocessed_frame = transform.resize(normalized_frame, [84, 84]) # Resize\n",
    "    return preprocessed_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some useful functions:\n",
    "\n",
    "* `np.argmax(v)`\n",
    "  * Return the index of the largest value in v\n",
    "* `np.stack(list)`\n",
    "  * Stack every element in list and convert the stacked list into a matrix\n",
    "* `np.expand_dims(m, axis=n)`\n",
    "  * Expand matrix m with an additional dimension along axis n\n",
    "* `game.new_episode()`\n",
    "  * Start a new episode\n",
    "* `game.get_state().screen_buffer`\n",
    "  * Screen buffer (image) of the current state of the game\n",
    "* `game.make_action(actions)`\n",
    "  * Take the actions denoted by True in the actions vector (e.g. [True, False, False] to perform action 0)\n",
    "* `game.is_episode_finished()`\n",
    "  * Returns `True` when the episode is done (terminal state or episode timeout)\n",
    "* `random.sample(list, n)`\n",
    "  * Sample n elements from list\n",
    "* `model.predict_on_batch(batch)`\n",
    "  * Feed batch through the network and return the output values (Q-values for the state)\n",
    "* `model.train_on_batch(batch, target)`\n",
    "  * Feed batch through the network and optimize network to predict \"target\" next time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Episode 500/500 --\n",
      "Episode loss: 707.8153312802315\n"
     ]
    }
   ],
   "source": [
    "if enable_training:\n",
    "    # Initialize replay buffer\n",
    "    replay_buffer = deque(maxlen=replay_buffer_size)\n",
    "\n",
    "    # Initialize frame stack\n",
    "    frame_stack = deque(maxlen=4)\n",
    "\n",
    "    # For every episode\n",
    "    episode_loss = float(\"nan\")\n",
    "    for episode in range(num_episodes):\n",
    "        clear_output(wait=True)\n",
    "        print(\"-- Episode {}/{} --\".format(episode+1, num_episodes))\n",
    "        print(\"Episode loss:\", episode_loss)\n",
    "\n",
    "        # Start new episode\n",
    "        game.new_episode()\n",
    "\n",
    "        # Initialize frame stack with the first frame of the game\n",
    "        initial_frame = preprocess_frame(game.get_state().screen_buffer)\n",
    "        for _ in range(4):\n",
    "            frame_stack.append(initial_frame)\n",
    "        state = np.stack(frame_stack, axis=2) # Stack the frames to setup the inital state\n",
    "\n",
    "        # Run the episode\n",
    "        episode_loss = 0\n",
    "        while not game.is_episode_finished():    \n",
    "            # Get action with highest Q-value for current state\n",
    "            action = np.argmax(model.predict_on_batch(np.expand_dims(state, axis=0)))\n",
    "            action_one_hot = [False] * num_actions\n",
    "            action_one_hot[action] = True\n",
    "\n",
    "            # Take action and get a reward\n",
    "            reward = game.make_action(action_one_hot)\n",
    "\n",
    "            # Break if the episode is finished\n",
    "            if game.is_episode_finished():\n",
    "                break\n",
    "\n",
    "            # If not, get the new state\n",
    "            frame_stack.append(preprocess_frame(game.get_state().screen_buffer))\n",
    "            new_state = np.stack(frame_stack, axis=2)\n",
    "\n",
    "            # Store the replay\n",
    "            replay_buffer.append((state, action, reward, new_state))\n",
    "            state = new_state\n",
    "\n",
    "            # Train network on a random sample of previous expreiences\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                # Get replay batch\n",
    "                replay_batch      = random.sample(replay_buffer, batch_size)\n",
    "                replay_state      = np.array([r[0] for r in replay_batch])\n",
    "                replay_reward     = np.array([r[2] for r in replay_batch])\n",
    "                replay_next_state = np.array([r[3] for r in replay_batch])\n",
    "\n",
    "                # Q_target = reward + gamma * max_a' Q(s', a')\n",
    "                Q_target = np.expand_dims(replay_reward, axis=1) + discount_factor * model.predict_on_batch(replay_next_state)\n",
    "\n",
    "                # Run training pass\n",
    "                episode_loss += model.train_on_batch(replay_state, Q_target)\n",
    "    model.save(\"basic_dqn.h5\")\n",
    "else:\n",
    "    model = load_model(\"basic_dqn.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f9ba203f1cb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[1;31m# If not, get the new state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mframe_stack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocess_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscreen_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe_stack\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-299824fbbde6>\u001b[0m in \u001b[0;36mpreprocess_frame\u001b[0;34m(frame)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcropped_frame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m]\u001b[0m                             \u001b[1;31m# Crop the screen\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mnormalized_frame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcropped_frame\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m255.0\u001b[0m                          \u001b[1;31m# Normalize pixel values\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mpreprocessed_frame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormalized_frame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m84\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m84\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Resize\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpreprocessed_frame\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\anaconda3\\lib\\site-packages\\skimage\\transform\\_warps.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(image, output_shape, order, mode, cval, clip, preserve_range)\u001b[0m\n\u001b[1;32m    117\u001b[0m         out = warp(image, tform, output_shape=output_shape, order=order,\n\u001b[1;32m    118\u001b[0m                    \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                    preserve_range=preserve_range)\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\anaconda3\\lib\\site-packages\\skimage\\transform\\_geometric.py\u001b[0m in \u001b[0;36mwarp\u001b[0;34m(image, inverse_map, map_args, output_shape, order, mode, cval, clip, preserve_range)\u001b[0m\n\u001b[1;32m   1341\u001b[0m                 warped = _warp_fast(image, matrix,\n\u001b[1;32m   1342\u001b[0m                                  \u001b[0moutput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m                                  order=order, mode=mode, cval=cval)\n\u001b[0m\u001b[1;32m   1344\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m                 \u001b[0mdims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mskimage/transform/_warps_cy.pyx\u001b[0m in \u001b[0;36mskimage.transform._warps_cy._warp_fast (skimage\\transform\\_warps_cy.c:2637)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mc:\\anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m \u001b[1;32mdef\u001b[0m \u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m     \"\"\"Convert the input to an array.\n\u001b[1;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Reinitialize game with set_window_visible = True\n",
    "game = vizdoom.DoomGame()\n",
    "game.load_config(\"scenarios/basic.cfg\")\n",
    "\n",
    "# Visualize the game\n",
    "game.set_window_visible(True)\n",
    "\n",
    "# Set screen format to greyscale. This improves training time\n",
    "game.set_screen_format(vizdoom.ScreenFormat.GRAY8)\n",
    "\n",
    "# Make the game end after 2100 ticks (set to 0 to disable)\n",
    "game.set_episode_timeout(100)\n",
    "\n",
    "# Init game\n",
    "game.init()\n",
    "\n",
    "record = True\n",
    "if record:\n",
    "    import imageio\n",
    "    recording = []\n",
    "\n",
    "# For every episode\n",
    "for episode in range(100):\n",
    "    # Start new episode\n",
    "    game.new_episode()\n",
    "    \n",
    "    # Initialize frame stack with the first frame of the game\n",
    "    initial_frame = preprocess_frame(game.get_state().screen_buffer)\n",
    "    for _ in range(4):\n",
    "        frame_stack.append(initial_frame)\n",
    "    state = np.stack(frame_stack, axis=2) # Stack the frames to setup the inital state\n",
    "    \n",
    "    # Run the episode\n",
    "    while not game.is_episode_finished():\n",
    "        if record:\n",
    "            recording.append((transform.resize(game.get_state().screen_buffer, [240, 320]) * 255.5).astype(np.uint8))\n",
    "        \n",
    "        # Get action with highest Q-value for current state\n",
    "        action = np.argmax(model.predict_on_batch(np.expand_dims(state, axis=0)))\n",
    "        action_one_hot = [False] * num_actions\n",
    "        action_one_hot[action] = True\n",
    "        \n",
    "        # Take action and get a reward\n",
    "        reward = game.make_action(action_one_hot)\n",
    "        \n",
    "        # Break if the episode is finished\n",
    "        if game.is_episode_finished():\n",
    "            break\n",
    "        \n",
    "        # If not, get the new state\n",
    "        frame_stack.append(preprocess_frame(game.get_state().screen_buffer))\n",
    "        state = np.stack(frame_stack, axis=2)\n",
    "        \n",
    "if record:\n",
    "    imageio.mimwrite(\"basic_dqn.gif\", recording, subrectangles=True)#, palettesize=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
